{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PD3_Regresja.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOIKkzdIr6iGCnvlci5UmBI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalSien/Michal-Lab/blob/main/PD3_Regresja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQH7zbVF8v6g"
      },
      "source": [
        "\n",
        "\n",
        "# **A regression example**\n",
        "\n",
        "Common type of machine learning problem is \"regression\", which consists of predicting a continuous value instead of a discrete label. For instance, predicting the temperature tomorrow, given meteorological data, or predicting the time that a software project will take to complete, given its specifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zl5s4feH_v4"
      },
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fEyq6reI8Vi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a068a8-ea5d-4d7c-b0d4-4695bfac6a0b"
      },
      "source": [
        "def funct(x):\n",
        "  return x*x*np.sin(x)+np.log(2*x)\n",
        "\n",
        "size = 200\n",
        "low=0\n",
        "high=12\n",
        "error=20\n",
        "\n",
        "XX_train = np.random.uniform(low=low, high=high, size=size)\n",
        "XX_test = np.random.uniform(low=low, high=high, size=size)\n",
        "yy_train = funct(XX_train) + np.random.normal(0., error, size)\n",
        "yy_test = funct(XX_test) + np.random.normal(0., error, size)\n",
        "\n",
        "print(XX_train.shape, yy_train.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200,) (200,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc4wlpu6NPib",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "4f1c253f-c6aa-46cd-a022-756a3e23a5f8"
      },
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.plot(XX_train,yy_train, 'o', color='blue', label='Training points')\n",
        "plt.plot(XX_test,yy_test, 'o', color='green', label='Testing points')\n",
        "\n",
        "points = np.linspace(low, high,num=100)\n",
        "plt.plot(points, funct(points),  color='red', label='Function')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS7hU2fiZHzl"
      },
      "source": [
        "\n",
        "**Preparing the data**\n",
        "\n",
        "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation. This is easily done in Numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNzLzYMFZNab"
      },
      "source": [
        "# Tutaj należy znormalizować dane, i.e. \n",
        "# - odjąć średnią\n",
        "# - podzielić przez odchylenie standardowe\n",
        "\n",
        "# \n",
        "mean = XX_train.mean(axis=0)\n",
        "XX_train_n = XX_train-mean\n",
        "std = XX_train_n.std(axis=0)\n",
        "XX_train_n /= std\n",
        "\n",
        "# to samo dla x_test\n",
        "XX_test_n = XX_test - mean\n",
        "XX_test_n /= std\n",
        "\n",
        "mean1 = yy_train.mean(axis=0)\n",
        "yy_train_n = yy_train-mean1\n",
        "std1 = yy_train_n.std(axis=0)\n",
        "yy_train_n /= std1\n",
        "\n",
        "# to samo dla y_test\n",
        "yy_test_n = yy_test - mean1\n",
        "yy_test_n /= std1\n",
        "\n",
        "# A teraz to samo dla yy_train i yy_test\n",
        "# dopisać to samo dla y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKnzCxTvWlLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd6c5b6-1a1b-45e0-83de-923ba0108903"
      },
      "source": [
        "# Zbudować sieć neuronową, działa: dwie warstwy ukryte, aktywacja relu\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model0 = Sequential(name='network')\n",
        "#.....\n",
        "model0.add(layers.Dense(1024, activation='relu', input_shape=(1,)))\n",
        "model0.add(Dropout(0.1))\n",
        "model0.add(layers.Dense(1024, activation='relu'))\n",
        "model0.add(Dropout(0.1))\n",
        "model0.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "model0.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"network\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1024)              2048      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 1,052,673\n",
            "Trainable params: 1,052,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9pQ61rwWX9F"
      },
      "source": [
        "Train neural network\n",
        "\n",
        "For regression problems, mean squared error (MSE) is often employed\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hos2Vg10XBU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ff74f9-f365-42ed-f1ea-aec90f297566"
      },
      "source": [
        "# compile and train NN\n",
        "# Uwaga: dla regresji używamy: loss = 'mean_squared_error'   oraz metrics = ['mse']\n",
        "opt=optimizers.RMSprop(lr=0.001)\n",
        "model0.compile(optimizer=opt,\n",
        "                       loss='mean_squared_error',\n",
        "                       metrics=['mse'])\n",
        "history = model0.fit(XX_train_n, yy_train_n,\n",
        "                                     epochs=500,\n",
        "                                     batch_size=128,\n",
        "                                     validation_data=(XX_test_n, yy_test_n),\n",
        "                                     verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 1s 484ms/step - loss: 1.1094 - mse: 1.1094 - val_loss: 0.8456 - val_mse: 0.8456\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.8024 - mse: 0.8024 - val_loss: 0.8549 - val_mse: 0.8549\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.7200 - mse: 0.7200 - val_loss: 0.8410 - val_mse: 0.8410\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.7456 - mse: 0.7456 - val_loss: 0.8420 - val_mse: 0.8420\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.6586 - mse: 0.6586 - val_loss: 0.7092 - val_mse: 0.7092\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.5740 - mse: 0.5740 - val_loss: 0.6392 - val_mse: 0.6392\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.5183 - mse: 0.5183 - val_loss: 0.6956 - val_mse: 0.6956\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.5256 - mse: 0.5256 - val_loss: 0.5481 - val_mse: 0.5481\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.4555 - mse: 0.4555 - val_loss: 0.6697 - val_mse: 0.6697\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.5725 - mse: 0.5725 - val_loss: 0.6418 - val_mse: 0.6418\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.5046 - mse: 0.5046 - val_loss: 0.5341 - val_mse: 0.5341\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.4282 - mse: 0.4282 - val_loss: 0.4807 - val_mse: 0.4807\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.3465 - mse: 0.3465 - val_loss: 0.4087 - val_mse: 0.4087\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.3468 - mse: 0.3468 - val_loss: 0.5295 - val_mse: 0.5295\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.3920 - mse: 0.3920 - val_loss: 0.4271 - val_mse: 0.4271\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.3332 - mse: 0.3332 - val_loss: 0.3437 - val_mse: 0.3437\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.2919 - mse: 0.2919 - val_loss: 0.3162 - val_mse: 0.3162\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.2788 - mse: 0.2788 - val_loss: 0.3174 - val_mse: 0.3174\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.3085 - mse: 0.3085 - val_loss: 0.4107 - val_mse: 0.4107\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.4079 - mse: 0.4079 - val_loss: 0.2904 - val_mse: 0.2904\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.2567 - mse: 0.2567 - val_loss: 0.2676 - val_mse: 0.2676\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.2260 - mse: 0.2260 - val_loss: 0.2587 - val_mse: 0.2587\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 0s 89ms/step - loss: 0.2155 - mse: 0.2155 - val_loss: 0.3046 - val_mse: 0.3046\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 0s 101ms/step - loss: 0.2330 - mse: 0.2330 - val_loss: 0.3382 - val_mse: 0.3382\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 0.2850 - mse: 0.2850 - val_loss: 0.3519 - val_mse: 0.3519\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 0.2603 - mse: 0.2603 - val_loss: 0.3692 - val_mse: 0.3692\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 0s 106ms/step - loss: 0.2836 - mse: 0.2836 - val_loss: 0.2621 - val_mse: 0.2621\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 0.2283 - mse: 0.2283 - val_loss: 0.3042 - val_mse: 0.3042\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 0.2192 - mse: 0.2192 - val_loss: 0.2608 - val_mse: 0.2608\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.2133 - mse: 0.2133 - val_loss: 0.2537 - val_mse: 0.2537\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.2194 - mse: 0.2194 - val_loss: 0.3443 - val_mse: 0.3443\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.2533 - mse: 0.2533 - val_loss: 0.2597 - val_mse: 0.2597\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 0s 150ms/step - loss: 0.2163 - mse: 0.2163 - val_loss: 0.2851 - val_mse: 0.2851\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.2178 - mse: 0.2178 - val_loss: 0.2436 - val_mse: 0.2436\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.1932 - mse: 0.1932 - val_loss: 0.2279 - val_mse: 0.2279\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1814 - mse: 0.1814 - val_loss: 0.1894 - val_mse: 0.1894\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.2001 - mse: 0.2001 - val_loss: 0.2558 - val_mse: 0.2558\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.3084 - mse: 0.3084 - val_loss: 0.2683 - val_mse: 0.2683\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.1934 - val_mse: 0.1934\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1691 - mse: 0.1691 - val_loss: 0.2064 - val_mse: 0.2064\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.2145 - val_mse: 0.2145\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1837 - mse: 0.1837 - val_loss: 0.2397 - val_mse: 0.2397\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1765 - mse: 0.1765 - val_loss: 0.2242 - val_mse: 0.2242\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.1777 - mse: 0.1777 - val_loss: 0.2398 - val_mse: 0.2398\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1896 - mse: 0.1896 - val_loss: 0.2869 - val_mse: 0.2869\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1840 - mse: 0.1840 - val_loss: 0.1888 - val_mse: 0.1888\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1694 - mse: 0.1694 - val_loss: 0.2005 - val_mse: 0.2005\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.1720 - mse: 0.1720 - val_loss: 0.4202 - val_mse: 0.4202\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.4632 - mse: 0.4632 - val_loss: 0.2241 - val_mse: 0.2241\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.2128 - mse: 0.2128 - val_loss: 0.2101 - val_mse: 0.2101\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1762 - mse: 0.1762 - val_loss: 0.1926 - val_mse: 0.1926\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1685 - mse: 0.1685 - val_loss: 0.1878 - val_mse: 0.1878\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1643 - mse: 0.1643 - val_loss: 0.1854 - val_mse: 0.1854\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 0s 90ms/step - loss: 0.1673 - mse: 0.1673 - val_loss: 0.1800 - val_mse: 0.1800\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1803 - mse: 0.1803 - val_loss: 0.1881 - val_mse: 0.1881\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1792 - mse: 0.1792 - val_loss: 0.2055 - val_mse: 0.2055\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.2130 - mse: 0.2130 - val_loss: 0.2244 - val_mse: 0.2244\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.2366 - mse: 0.2366 - val_loss: 0.1801 - val_mse: 0.1801\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.1678 - mse: 0.1678 - val_loss: 0.1823 - val_mse: 0.1823\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.1659 - mse: 0.1659 - val_loss: 0.1934 - val_mse: 0.1934\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1628 - mse: 0.1628 - val_loss: 0.1827 - val_mse: 0.1827\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1507 - mse: 0.1507 - val_loss: 0.1920 - val_mse: 0.1920\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1632 - mse: 0.1632 - val_loss: 0.1925 - val_mse: 0.1925\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1641 - mse: 0.1641 - val_loss: 0.2946 - val_mse: 0.2946\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.2690 - mse: 0.2690 - val_loss: 0.3277 - val_mse: 0.3277\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.2510 - mse: 0.2510 - val_loss: 0.2318 - val_mse: 0.2318\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1873 - mse: 0.1873 - val_loss: 0.2093 - val_mse: 0.2093\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1751 - mse: 0.1751 - val_loss: 0.2008 - val_mse: 0.2008\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1669 - mse: 0.1669 - val_loss: 0.2267 - val_mse: 0.2267\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1915 - mse: 0.1915 - val_loss: 0.2341 - val_mse: 0.2341\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1972 - mse: 0.1972 - val_loss: 0.2340 - val_mse: 0.2340\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1774 - mse: 0.1774 - val_loss: 0.2112 - val_mse: 0.2112\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 0s 164ms/step - loss: 0.1583 - mse: 0.1583 - val_loss: 0.1812 - val_mse: 0.1812\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.1586 - mse: 0.1586 - val_loss: 0.1798 - val_mse: 0.1798\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1574 - mse: 0.1574 - val_loss: 0.1972 - val_mse: 0.1972\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1622 - mse: 0.1622 - val_loss: 0.2559 - val_mse: 0.2559\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.2276 - mse: 0.2276 - val_loss: 0.2526 - val_mse: 0.2526\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.2150 - mse: 0.2150 - val_loss: 0.2412 - val_mse: 0.2412\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1864 - mse: 0.1864 - val_loss: 0.1957 - val_mse: 0.1957\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1697 - mse: 0.1697 - val_loss: 0.2022 - val_mse: 0.2022\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1987 - val_mse: 0.1987\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1806 - mse: 0.1806 - val_loss: 0.1753 - val_mse: 0.1753\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.1572 - mse: 0.1572 - val_loss: 0.2054 - val_mse: 0.2054\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1492 - mse: 0.1492 - val_loss: 0.1719 - val_mse: 0.1719\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1731 - mse: 0.1731 - val_loss: 0.2007 - val_mse: 0.2007\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1930 - mse: 0.1930 - val_loss: 0.2284 - val_mse: 0.2284\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.2652 - mse: 0.2652 - val_loss: 0.1972 - val_mse: 0.1972\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1808 - mse: 0.1808 - val_loss: 0.1722 - val_mse: 0.1722\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1598 - mse: 0.1598 - val_loss: 0.1811 - val_mse: 0.1811\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1608 - mse: 0.1608 - val_loss: 0.1796 - val_mse: 0.1796\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1588 - mse: 0.1588 - val_loss: 0.1749 - val_mse: 0.1749\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1499 - mse: 0.1499 - val_loss: 0.1947 - val_mse: 0.1947\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1793 - val_mse: 0.1793\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.1597 - mse: 0.1597 - val_loss: 0.2524 - val_mse: 0.2524\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.1818 - mse: 0.1818 - val_loss: 0.2361 - val_mse: 0.2361\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1888 - mse: 0.1888 - val_loss: 0.2091 - val_mse: 0.2091\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1797 - val_mse: 0.1797\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1761 - mse: 0.1761 - val_loss: 0.1753 - val_mse: 0.1753\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1544 - mse: 0.1544 - val_loss: 0.2021 - val_mse: 0.2021\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1559 - mse: 0.1559 - val_loss: 0.1784 - val_mse: 0.1784\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 0.1785 - val_mse: 0.1785\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1610 - mse: 0.1610 - val_loss: 0.2249 - val_mse: 0.2249\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.2070 - mse: 0.2070 - val_loss: 0.1809 - val_mse: 0.1809\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.1559 - mse: 0.1559 - val_loss: 0.1697 - val_mse: 0.1697\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1757 - mse: 0.1757 - val_loss: 0.1791 - val_mse: 0.1791\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1714 - mse: 0.1714 - val_loss: 0.1877 - val_mse: 0.1877\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1661 - mse: 0.1661 - val_loss: 0.1784 - val_mse: 0.1784\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1556 - mse: 0.1556 - val_loss: 0.1835 - val_mse: 0.1835\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1570 - mse: 0.1570 - val_loss: 0.1831 - val_mse: 0.1831\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1536 - mse: 0.1536 - val_loss: 0.2024 - val_mse: 0.2024\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.2530 - val_mse: 0.2530\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1705 - mse: 0.1705 - val_loss: 0.2310 - val_mse: 0.2310\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1761 - mse: 0.1761 - val_loss: 0.2307 - val_mse: 0.2307\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1809 - mse: 0.1809 - val_loss: 0.1943 - val_mse: 0.1943\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1549 - mse: 0.1549 - val_loss: 0.2168 - val_mse: 0.2168\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.2070 - mse: 0.2070 - val_loss: 0.3001 - val_mse: 0.3001\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.2265 - mse: 0.2265 - val_loss: 0.1753 - val_mse: 0.1753\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1522 - mse: 0.1522 - val_loss: 0.1947 - val_mse: 0.1947\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1779 - mse: 0.1779 - val_loss: 0.1754 - val_mse: 0.1754\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1458 - mse: 0.1458 - val_loss: 0.1825 - val_mse: 0.1825\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1559 - mse: 0.1559 - val_loss: 0.2182 - val_mse: 0.2182\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1767 - mse: 0.1767 - val_loss: 0.2358 - val_mse: 0.2358\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 0s 174ms/step - loss: 0.1756 - mse: 0.1756 - val_loss: 0.1699 - val_mse: 0.1699\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1488 - mse: 0.1488 - val_loss: 0.1713 - val_mse: 0.1713\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 0.1819 - val_mse: 0.1819\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1594 - mse: 0.1594 - val_loss: 0.1903 - val_mse: 0.1903\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1536 - mse: 0.1536 - val_loss: 0.2310 - val_mse: 0.2310\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.1759 - mse: 0.1759 - val_loss: 0.2093 - val_mse: 0.2093\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1569 - mse: 0.1569 - val_loss: 0.1702 - val_mse: 0.1702\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1498 - mse: 0.1498 - val_loss: 0.1784 - val_mse: 0.1784\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1490 - mse: 0.1490 - val_loss: 0.3060 - val_mse: 0.3060\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.2290 - mse: 0.2290 - val_loss: 0.2158 - val_mse: 0.2158\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1583 - mse: 0.1583 - val_loss: 0.1830 - val_mse: 0.1830\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1475 - mse: 0.1475 - val_loss: 0.1836 - val_mse: 0.1836\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1508 - mse: 0.1508 - val_loss: 0.1932 - val_mse: 0.1932\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1656 - mse: 0.1656 - val_loss: 0.2488 - val_mse: 0.2488\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.1905 - mse: 0.1905 - val_loss: 0.1843 - val_mse: 0.1843\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1526 - mse: 0.1526 - val_loss: 0.1922 - val_mse: 0.1922\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1766 - mse: 0.1766 - val_loss: 0.2548 - val_mse: 0.2548\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.1738 - val_mse: 0.1738\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1512 - mse: 0.1512 - val_loss: 0.1960 - val_mse: 0.1960\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1654 - mse: 0.1654 - val_loss: 0.1953 - val_mse: 0.1953\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1547 - mse: 0.1547 - val_loss: 0.1671 - val_mse: 0.1671\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.1433 - mse: 0.1433 - val_loss: 0.1868 - val_mse: 0.1868\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1604 - mse: 0.1604 - val_loss: 0.1641 - val_mse: 0.1641\n",
            "Epoch 146/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1449 - mse: 0.1449 - val_loss: 0.1710 - val_mse: 0.1710\n",
            "Epoch 147/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1395 - mse: 0.1395 - val_loss: 0.1842 - val_mse: 0.1842\n",
            "Epoch 148/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1497 - mse: 0.1497 - val_loss: 0.2617 - val_mse: 0.2617\n",
            "Epoch 149/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.2344 - mse: 0.2344 - val_loss: 0.1800 - val_mse: 0.1800\n",
            "Epoch 150/500\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.1573 - mse: 0.1573 - val_loss: 0.1776 - val_mse: 0.1776\n",
            "Epoch 151/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1463 - mse: 0.1463 - val_loss: 0.1771 - val_mse: 0.1771\n",
            "Epoch 152/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.1357 - mse: 0.1357 - val_loss: 0.2166 - val_mse: 0.2166\n",
            "Epoch 153/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.1727 - mse: 0.1727 - val_loss: 0.1922 - val_mse: 0.1922\n",
            "Epoch 154/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1543 - mse: 0.1543 - val_loss: 0.2061 - val_mse: 0.2061\n",
            "Epoch 155/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1594 - mse: 0.1594 - val_loss: 0.1717 - val_mse: 0.1717\n",
            "Epoch 156/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1456 - mse: 0.1456 - val_loss: 0.1686 - val_mse: 0.1686\n",
            "Epoch 157/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1379 - mse: 0.1379 - val_loss: 0.1993 - val_mse: 0.1993\n",
            "Epoch 158/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1487 - mse: 0.1487 - val_loss: 0.2036 - val_mse: 0.2036\n",
            "Epoch 159/500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.1432 - mse: 0.1432 - val_loss: 0.1763 - val_mse: 0.1763\n",
            "Epoch 160/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1396 - mse: 0.1396 - val_loss: 0.1798 - val_mse: 0.1798\n",
            "Epoch 161/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1373 - mse: 0.1373 - val_loss: 0.1857 - val_mse: 0.1857\n",
            "Epoch 162/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1741 - mse: 0.1741 - val_loss: 0.1957 - val_mse: 0.1957\n",
            "Epoch 163/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1645 - mse: 0.1645 - val_loss: 0.1713 - val_mse: 0.1713\n",
            "Epoch 164/500\n",
            "2/2 [==============================] - 0s 153ms/step - loss: 0.1495 - mse: 0.1495 - val_loss: 0.1805 - val_mse: 0.1805\n",
            "Epoch 165/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1572 - mse: 0.1572 - val_loss: 0.1741 - val_mse: 0.1741\n",
            "Epoch 166/500\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 0.1536 - mse: 0.1536 - val_loss: 0.2291 - val_mse: 0.2291\n",
            "Epoch 167/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1737 - mse: 0.1737 - val_loss: 0.2223 - val_mse: 0.2223\n",
            "Epoch 168/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1697 - mse: 0.1697 - val_loss: 0.1661 - val_mse: 0.1661\n",
            "Epoch 169/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1482 - mse: 0.1482 - val_loss: 0.2031 - val_mse: 0.2031\n",
            "Epoch 170/500\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.1635 - mse: 0.1635 - val_loss: 0.1829 - val_mse: 0.1829\n",
            "Epoch 171/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1427 - mse: 0.1427 - val_loss: 0.1768 - val_mse: 0.1768\n",
            "Epoch 172/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1380 - mse: 0.1380 - val_loss: 0.1739 - val_mse: 0.1739\n",
            "Epoch 173/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1457 - mse: 0.1457 - val_loss: 0.1767 - val_mse: 0.1767\n",
            "Epoch 174/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1440 - mse: 0.1440 - val_loss: 0.1878 - val_mse: 0.1878\n",
            "Epoch 175/500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1643 - mse: 0.1643 - val_loss: 0.2589 - val_mse: 0.2589\n",
            "Epoch 176/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1856 - mse: 0.1856 - val_loss: 0.1869 - val_mse: 0.1869\n",
            "Epoch 177/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1538 - mse: 0.1538 - val_loss: 0.1751 - val_mse: 0.1751\n",
            "Epoch 178/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1487 - mse: 0.1487 - val_loss: 0.1940 - val_mse: 0.1940\n",
            "Epoch 179/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1658 - mse: 0.1658 - val_loss: 0.1842 - val_mse: 0.1842\n",
            "Epoch 180/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1553 - mse: 0.1553 - val_loss: 0.1996 - val_mse: 0.1996\n",
            "Epoch 181/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1574 - mse: 0.1574 - val_loss: 0.1889 - val_mse: 0.1889\n",
            "Epoch 182/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1478 - mse: 0.1478 - val_loss: 0.1895 - val_mse: 0.1895\n",
            "Epoch 183/500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.1607 - mse: 0.1607 - val_loss: 0.2357 - val_mse: 0.2357\n",
            "Epoch 184/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1837 - mse: 0.1837 - val_loss: 0.1883 - val_mse: 0.1883\n",
            "Epoch 185/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1545 - mse: 0.1545 - val_loss: 0.1804 - val_mse: 0.1804\n",
            "Epoch 186/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1431 - mse: 0.1431 - val_loss: 0.1819 - val_mse: 0.1819\n",
            "Epoch 187/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1511 - mse: 0.1511 - val_loss: 0.1645 - val_mse: 0.1645\n",
            "Epoch 188/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1522 - mse: 0.1522 - val_loss: 0.1895 - val_mse: 0.1895\n",
            "Epoch 189/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1614 - mse: 0.1614 - val_loss: 0.1827 - val_mse: 0.1827\n",
            "Epoch 190/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1593 - mse: 0.1593 - val_loss: 0.1647 - val_mse: 0.1647\n",
            "Epoch 191/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.1388 - mse: 0.1388 - val_loss: 0.1796 - val_mse: 0.1796\n",
            "Epoch 192/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1413 - mse: 0.1413 - val_loss: 0.1726 - val_mse: 0.1726\n",
            "Epoch 193/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 0.1772 - val_mse: 0.1772\n",
            "Epoch 194/500\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.1421 - mse: 0.1421 - val_loss: 0.1741 - val_mse: 0.1741\n",
            "Epoch 195/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1427 - mse: 0.1427 - val_loss: 0.2577 - val_mse: 0.2577\n",
            "Epoch 196/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.2116 - mse: 0.2116 - val_loss: 0.1967 - val_mse: 0.1967\n",
            "Epoch 197/500\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 0.1537 - mse: 0.1537 - val_loss: 0.1827 - val_mse: 0.1827\n",
            "Epoch 198/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1479 - mse: 0.1479 - val_loss: 0.2058 - val_mse: 0.2058\n",
            "Epoch 199/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1660 - mse: 0.1660 - val_loss: 0.2096 - val_mse: 0.2096\n",
            "Epoch 200/500\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1689 - val_mse: 0.1689\n",
            "Epoch 201/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1380 - mse: 0.1380 - val_loss: 0.2088 - val_mse: 0.2088\n",
            "Epoch 202/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1640 - mse: 0.1640 - val_loss: 0.1921 - val_mse: 0.1921\n",
            "Epoch 203/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1453 - mse: 0.1453 - val_loss: 0.1781 - val_mse: 0.1781\n",
            "Epoch 204/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1404 - mse: 0.1404 - val_loss: 0.1735 - val_mse: 0.1735\n",
            "Epoch 205/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1371 - mse: 0.1371 - val_loss: 0.1774 - val_mse: 0.1774\n",
            "Epoch 206/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1428 - mse: 0.1428 - val_loss: 0.1701 - val_mse: 0.1701\n",
            "Epoch 207/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1459 - mse: 0.1459 - val_loss: 0.1765 - val_mse: 0.1765\n",
            "Epoch 208/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.1498 - mse: 0.1498 - val_loss: 0.1921 - val_mse: 0.1921\n",
            "Epoch 209/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1418 - mse: 0.1418 - val_loss: 0.1867 - val_mse: 0.1867\n",
            "Epoch 210/500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.1757 - mse: 0.1757 - val_loss: 0.2078 - val_mse: 0.2078\n",
            "Epoch 211/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1696 - mse: 0.1696 - val_loss: 0.1709 - val_mse: 0.1709\n",
            "Epoch 212/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1428 - mse: 0.1428 - val_loss: 0.1775 - val_mse: 0.1775\n",
            "Epoch 213/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1416 - mse: 0.1416 - val_loss: 0.1668 - val_mse: 0.1668\n",
            "Epoch 214/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1307 - mse: 0.1307 - val_loss: 0.1855 - val_mse: 0.1855\n",
            "Epoch 215/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1535 - mse: 0.1535 - val_loss: 0.1902 - val_mse: 0.1902\n",
            "Epoch 216/500\n",
            "2/2 [==============================] - 0s 167ms/step - loss: 0.1544 - mse: 0.1544 - val_loss: 0.1687 - val_mse: 0.1687\n",
            "Epoch 217/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1411 - mse: 0.1411 - val_loss: 0.1802 - val_mse: 0.1802\n",
            "Epoch 218/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.1382 - mse: 0.1382 - val_loss: 0.1916 - val_mse: 0.1916\n",
            "Epoch 219/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1384 - mse: 0.1384 - val_loss: 0.1663 - val_mse: 0.1663\n",
            "Epoch 220/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1469 - mse: 0.1469 - val_loss: 0.2011 - val_mse: 0.2011\n",
            "Epoch 221/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1538 - mse: 0.1538 - val_loss: 0.1880 - val_mse: 0.1880\n",
            "Epoch 222/500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1543 - mse: 0.1543 - val_loss: 0.1981 - val_mse: 0.1981\n",
            "Epoch 223/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1755 - mse: 0.1755 - val_loss: 0.1615 - val_mse: 0.1615\n",
            "Epoch 224/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1479 - mse: 0.1479 - val_loss: 0.1661 - val_mse: 0.1661\n",
            "Epoch 225/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1328 - mse: 0.1328 - val_loss: 0.1811 - val_mse: 0.1811\n",
            "Epoch 226/500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.1544 - mse: 0.1544 - val_loss: 0.1842 - val_mse: 0.1842\n",
            "Epoch 227/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1529 - mse: 0.1529 - val_loss: 0.1777 - val_mse: 0.1777\n",
            "Epoch 228/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1680 - mse: 0.1680 - val_loss: 0.1849 - val_mse: 0.1849\n",
            "Epoch 229/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1473 - mse: 0.1473 - val_loss: 0.1757 - val_mse: 0.1757\n",
            "Epoch 230/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1373 - mse: 0.1373 - val_loss: 0.1963 - val_mse: 0.1963\n",
            "Epoch 231/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1404 - mse: 0.1404 - val_loss: 0.1761 - val_mse: 0.1761\n",
            "Epoch 232/500\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1461 - mse: 0.1461 - val_loss: 0.1765 - val_mse: 0.1765\n",
            "Epoch 233/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1374 - mse: 0.1374 - val_loss: 0.1801 - val_mse: 0.1801\n",
            "Epoch 234/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1523 - mse: 0.1523 - val_loss: 0.2319 - val_mse: 0.2319\n",
            "Epoch 235/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1818 - mse: 0.1818 - val_loss: 0.1899 - val_mse: 0.1899\n",
            "Epoch 236/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1560 - mse: 0.1560 - val_loss: 0.1818 - val_mse: 0.1818\n",
            "Epoch 237/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1407 - mse: 0.1407 - val_loss: 0.1753 - val_mse: 0.1753\n",
            "Epoch 238/500\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1413 - mse: 0.1413 - val_loss: 0.1731 - val_mse: 0.1731\n",
            "Epoch 239/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1365 - mse: 0.1365 - val_loss: 0.1720 - val_mse: 0.1720\n",
            "Epoch 240/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.1421 - mse: 0.1421 - val_loss: 0.1725 - val_mse: 0.1725\n",
            "Epoch 241/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1287 - mse: 0.1287 - val_loss: 0.1874 - val_mse: 0.1874\n",
            "Epoch 242/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1632 - mse: 0.1632 - val_loss: 0.1692 - val_mse: 0.1692\n",
            "Epoch 243/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1434 - mse: 0.1434 - val_loss: 0.1771 - val_mse: 0.1771\n",
            "Epoch 244/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1440 - mse: 0.1440 - val_loss: 0.1891 - val_mse: 0.1891\n",
            "Epoch 245/500\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.1486 - mse: 0.1486 - val_loss: 0.1837 - val_mse: 0.1837\n",
            "Epoch 246/500\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1557 - mse: 0.1557 - val_loss: 0.1772 - val_mse: 0.1772\n",
            "Epoch 247/500\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.1380 - mse: 0.1380 - val_loss: 0.1793 - val_mse: 0.1793\n",
            "Epoch 248/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1348 - mse: 0.1348 - val_loss: 0.1871 - val_mse: 0.1871\n",
            "Epoch 249/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1463 - mse: 0.1463 - val_loss: 0.2008 - val_mse: 0.2008\n",
            "Epoch 250/500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.1795 - mse: 0.1795 - val_loss: 0.2034 - val_mse: 0.2034\n",
            "Epoch 251/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1787 - mse: 0.1787 - val_loss: 0.1826 - val_mse: 0.1826\n",
            "Epoch 252/500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.1454 - mse: 0.1454 - val_loss: 0.1743 - val_mse: 0.1743\n",
            "Epoch 253/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1358 - mse: 0.1358 - val_loss: 0.1768 - val_mse: 0.1768\n",
            "Epoch 254/500\n",
            "2/2 [==============================] - 0s 84ms/step - loss: 0.1381 - mse: 0.1381 - val_loss: 0.1791 - val_mse: 0.1791\n",
            "Epoch 255/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1466 - mse: 0.1466 - val_loss: 0.1774 - val_mse: 0.1774\n",
            "Epoch 256/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1321 - mse: 0.1321 - val_loss: 0.1763 - val_mse: 0.1763\n",
            "Epoch 257/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1486 - mse: 0.1486 - val_loss: 0.1814 - val_mse: 0.1814\n",
            "Epoch 258/500\n",
            "2/2 [==============================] - 0s 179ms/step - loss: 0.1475 - mse: 0.1475 - val_loss: 0.1841 - val_mse: 0.1841\n",
            "Epoch 259/500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1511 - mse: 0.1511 - val_loss: 0.2244 - val_mse: 0.2244\n",
            "Epoch 260/500\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.1765 - mse: 0.1765 - val_loss: 0.1930 - val_mse: 0.1930\n",
            "Epoch 261/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1558 - mse: 0.1558 - val_loss: 0.1815 - val_mse: 0.1815\n",
            "Epoch 262/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1474 - mse: 0.1474 - val_loss: 0.1964 - val_mse: 0.1964\n",
            "Epoch 263/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1494 - mse: 0.1494 - val_loss: 0.1701 - val_mse: 0.1701\n",
            "Epoch 264/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1389 - mse: 0.1389 - val_loss: 0.1969 - val_mse: 0.1969\n",
            "Epoch 265/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1358 - mse: 0.1358 - val_loss: 0.1903 - val_mse: 0.1903\n",
            "Epoch 266/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1566 - mse: 0.1566 - val_loss: 0.1934 - val_mse: 0.1934\n",
            "Epoch 267/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1807 - mse: 0.1807 - val_loss: 0.1920 - val_mse: 0.1920\n",
            "Epoch 268/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1569 - mse: 0.1569 - val_loss: 0.1722 - val_mse: 0.1722\n",
            "Epoch 269/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1470 - mse: 0.1470 - val_loss: 0.1739 - val_mse: 0.1739\n",
            "Epoch 270/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1330 - mse: 0.1330 - val_loss: 0.1752 - val_mse: 0.1752\n",
            "Epoch 271/500\n",
            "2/2 [==============================] - 0s 90ms/step - loss: 0.1378 - mse: 0.1378 - val_loss: 0.1786 - val_mse: 0.1786\n",
            "Epoch 272/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1471 - mse: 0.1471 - val_loss: 0.1678 - val_mse: 0.1678\n",
            "Epoch 273/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1394 - mse: 0.1394 - val_loss: 0.1803 - val_mse: 0.1803\n",
            "Epoch 274/500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1425 - mse: 0.1425 - val_loss: 0.1838 - val_mse: 0.1838\n",
            "Epoch 275/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1350 - mse: 0.1350 - val_loss: 0.1731 - val_mse: 0.1731\n",
            "Epoch 276/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1439 - mse: 0.1439 - val_loss: 0.1868 - val_mse: 0.1868\n",
            "Epoch 277/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1548 - mse: 0.1548 - val_loss: 0.2093 - val_mse: 0.2093\n",
            "Epoch 278/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1651 - mse: 0.1651 - val_loss: 0.1812 - val_mse: 0.1812\n",
            "Epoch 279/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1389 - mse: 0.1389 - val_loss: 0.1744 - val_mse: 0.1744\n",
            "Epoch 280/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1440 - mse: 0.1440 - val_loss: 0.1826 - val_mse: 0.1826\n",
            "Epoch 281/500\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 0.1461 - mse: 0.1461 - val_loss: 0.1821 - val_mse: 0.1821\n",
            "Epoch 282/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1381 - mse: 0.1381 - val_loss: 0.1760 - val_mse: 0.1760\n",
            "Epoch 283/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1538 - mse: 0.1538 - val_loss: 0.2373 - val_mse: 0.2373\n",
            "Epoch 284/500\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1695 - mse: 0.1695 - val_loss: 0.1872 - val_mse: 0.1872\n",
            "Epoch 285/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1433 - mse: 0.1433 - val_loss: 0.1842 - val_mse: 0.1842\n",
            "Epoch 286/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1376 - mse: 0.1376 - val_loss: 0.1621 - val_mse: 0.1621\n",
            "Epoch 287/500\n",
            "2/2 [==============================] - 0s 80ms/step - loss: 0.1365 - mse: 0.1365 - val_loss: 0.1811 - val_mse: 0.1811\n",
            "Epoch 288/500\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.1305 - mse: 0.1305 - val_loss: 0.1717 - val_mse: 0.1717\n",
            "Epoch 289/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1310 - mse: 0.1310 - val_loss: 0.2186 - val_mse: 0.2186\n",
            "Epoch 290/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.1719 - mse: 0.1719 - val_loss: 0.1711 - val_mse: 0.1711\n",
            "Epoch 291/500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1371 - mse: 0.1371 - val_loss: 0.2000 - val_mse: 0.2000\n",
            "Epoch 292/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1314 - mse: 0.1314 - val_loss: 0.1585 - val_mse: 0.1585\n",
            "Epoch 293/500\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1304 - mse: 0.1304 - val_loss: 0.1743 - val_mse: 0.1743\n",
            "Epoch 294/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1333 - mse: 0.1333 - val_loss: 0.1775 - val_mse: 0.1775\n",
            "Epoch 295/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1391 - mse: 0.1391 - val_loss: 0.1823 - val_mse: 0.1823\n",
            "Epoch 296/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1551 - mse: 0.1551 - val_loss: 0.2017 - val_mse: 0.2017\n",
            "Epoch 297/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1685 - mse: 0.1685 - val_loss: 0.1848 - val_mse: 0.1848\n",
            "Epoch 298/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1506 - mse: 0.1506 - val_loss: 0.1893 - val_mse: 0.1893\n",
            "Epoch 299/500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1436 - mse: 0.1436 - val_loss: 0.1720 - val_mse: 0.1720\n",
            "Epoch 300/500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1406 - mse: 0.1406 - val_loss: 0.1845 - val_mse: 0.1845\n",
            "Epoch 301/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1455 - mse: 0.1455 - val_loss: 0.1740 - val_mse: 0.1740\n",
            "Epoch 302/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1403 - mse: 0.1403 - val_loss: 0.1738 - val_mse: 0.1738\n",
            "Epoch 303/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1425 - mse: 0.1425 - val_loss: 0.1631 - val_mse: 0.1631\n",
            "Epoch 304/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1365 - mse: 0.1365 - val_loss: 0.1686 - val_mse: 0.1686\n",
            "Epoch 305/500\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.1440 - mse: 0.1440 - val_loss: 0.1876 - val_mse: 0.1876\n",
            "Epoch 306/500\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1486 - mse: 0.1486 - val_loss: 0.1908 - val_mse: 0.1908\n",
            "Epoch 307/500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1520 - mse: 0.1520 - val_loss: 0.1714 - val_mse: 0.1714\n",
            "Epoch 308/500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1399 - mse: 0.1399"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksInevx1gt33"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 1.5])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error ')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhWj-M9sWa1A"
      },
      "source": [
        "Evaluate neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGobBA6XfTt"
      },
      "source": [
        "results = model0.evaluate(XX_test_n, yy_test_n)\n",
        "\n",
        "print('loss test data: ', results[0])\n",
        "print('mse test data: ', results[1])\n",
        "\n",
        "results = model0.evaluate(XX_train_n, yy_train_n)\n",
        "\n",
        "print('loss train data: ', results[0])\n",
        "print('mse train data: ', results[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwHkHPnzvD_z"
      },
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.plot(XX_train_n,yy_train_n, 'o', color='blue', label='Training points')\n",
        "plt.plot(XX_test_n,yy_test_n, 'o', color='green', label='Testing points')\n",
        "\n",
        "points = np.linspace(min(XX_test_n), max(XX_test_n),num=100)\n",
        "plt.plot(points, (funct(points*std+mean)-mean1)/std1,  color='red', label='Function')\n",
        "\n",
        "plt.plot(points, model0.predict(points),  color='orange', label='Neural net')\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDZznlELK_xZ"
      },
      "source": [
        "## **Regularizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7U9XEvNIILs"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGM1djJmIJZz"
      },
      "source": [
        "def funct(x):\n",
        "  return x*x*np.sin(x)+np.log(2*x)\n",
        "\n",
        "size = 150\n",
        "low=0\n",
        "high=12\n",
        "error=5\n",
        "\n",
        "XX_train = np.random.uniform(low=low, high=high, size=size)\n",
        "XX_test = np.random.uniform(low=low, high=high, size=size)\n",
        "yy_train = funct(XX_train) + np.random.normal(0., error, size)\n",
        "yy_test = funct(XX_test) + np.random.normal(0., error, size)\n",
        "\n",
        "print(XX_train.shape, yy_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnbPrb13IJ3e"
      },
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.plot(XX_train,yy_train, 'o', color='blue', label='Training points')\n",
        "plt.plot(XX_test,yy_test, 'o', color='green', label='Testing points')\n",
        "\n",
        "points = np.linspace(low, high,num=100)\n",
        "plt.plot(points, funct(points),  color='red', label='Function')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwTdCGtoIKix"
      },
      "source": [
        "# Tutaj należy znormalizować dane, i.e. \n",
        "# - odjąć średnią\n",
        "# - podzielić przez odchylenie standardowe\n",
        "\n",
        "# \n",
        "mean = XX_train.mean(axis=0)\n",
        "XX_train_n = XX_train-mean\n",
        "std = XX_train_n.std(axis=0)\n",
        "XX_train_n /= std\n",
        "\n",
        "# to samo dla x_test\n",
        "XX_test_n = XX_test - mean\n",
        "XX_test_n /= std\n",
        "\n",
        "mean1 = yy_train.mean(axis=0)\n",
        "yy_train_n = yy_train-mean1\n",
        "std1 = yy_train_n.std(axis=0)\n",
        "yy_train_n /= std1\n",
        "\n",
        "# to samo dla y_test\n",
        "yy_test_n = yy_test - mean1\n",
        "yy_test_n /= std1\n",
        "\n",
        "# A teraz to samo dla yy_train i yy_test\n",
        "# dopisać to samo dla y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J4B2QO5IK_2"
      },
      "source": [
        "# Zbudować sieć neuronową, działa: dwie warstwy ukryte, aktywacja relu\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "model0 = Sequential(name='network')\n",
        "#.....\n",
        "model0.add(layers.Dense(1024,kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(1,)))\n",
        "model0.add(layers.Dense(1024,kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
        "model0.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "model0.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CWDomv2ILjv"
      },
      "source": [
        "# compile and train NN\n",
        "# Uwaga: dla regresji używamy: loss = 'mean_squared_error'   oraz metrics = ['mse']\n",
        "opt=optimizers.RMSprop(lr=0.002)\n",
        "model0.compile(optimizer=opt,\n",
        "                       loss='mean_squared_error',\n",
        "                       metrics=['mse'])\n",
        "history = model0.fit(XX_train_n, yy_train_n,\n",
        "                                     epochs=600,\n",
        "                                     batch_size=128,\n",
        "                                     validation_data=(XX_test_n, yy_test_n),\n",
        "                                     verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XknbAQHqJTFu"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 1.5])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error ')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKqb0xd9JT89"
      },
      "source": [
        "results = model0.evaluate(XX_test_n, yy_test_n)\n",
        "\n",
        "print('loss test data: ', results[0])\n",
        "print('mse test data: ', results[1])\n",
        "\n",
        "results = model0.evaluate(XX_train_n, yy_train_n)\n",
        "\n",
        "print('loss train data: ', results[0])\n",
        "print('mse train data: ', results[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjVxl869JUdA"
      },
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.plot(XX_train_n,yy_train_n, 'o', color='blue', label='Training points')\n",
        "plt.plot(XX_test_n,yy_test_n, 'o', color='green', label='Testing points')\n",
        "\n",
        "points = np.linspace(min(XX_test_n), max(XX_test_n),num=100)\n",
        "plt.plot(points, (funct(points*std+mean)-mean1)/std1,  color='red', label='Function')\n",
        "\n",
        "plt.plot(points, model0.predict(points),  color='orange', label='Neural net')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnwcHVfaHJif"
      },
      "source": [
        "**The Boston Housing Price dataset**\n",
        "\n",
        "We will be attempting to predict the median price of homes in a given Boston suburb in the mid-1970s, given a few data points about the suburb at the time, such as the crime rate, the local property tax rate, etc.\n",
        "\n",
        "The dataset has very few data points, only 506 in total, split between 404 training samples and 102 test samples, and each \"feature\" in the input data (e.g. the crime rate is a feature) has a different scale. For instance some values are proportions, which take a values between 0 and 1, others take values between 1 and 12, others between 0 and 100..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__lqdBmD9cgL"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDCcGuFW9RP6"
      },
      "source": [
        "\n",
        "**Preparing the data**\n",
        "\n",
        "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation. This is easily done in Numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRPumcs09RP7"
      },
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "\n",
        "plt.plot(XX_train,yy_train, 'o', color='blue', label='Training points')\n",
        "plt.plot(XX_test,yy_test, 'o', color='green', label='Testing points')\n",
        "\n",
        "points = np.linspace(low, high,num=100)\n",
        "plt.plot(points, funct(points),  color='red', label='Function')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuj5f4Mg6e5y"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86NDw6Qm91dC"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 0.5])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [MPG]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2enpb6gFGte"
      },
      "source": [
        "Evaluate neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dewsTB7XbAh"
      },
      "source": [
        "results = model.evaluate(X_test_n, y_test_n)\n",
        "\n",
        "print('loss test data: ', results[0])\n",
        "print('mse test data: ', results[1])\n",
        "\n",
        "results = model.evaluate(X_train_n, y_train_n)\n",
        "\n",
        "print('loss train data: ', results[0])\n",
        "print('mse train data: ', results[1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}